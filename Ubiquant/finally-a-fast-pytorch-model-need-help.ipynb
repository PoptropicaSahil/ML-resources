{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Welcome to Pytorch!\nAfter almost everyone using Keras to get good scores in the competition, I took the challenge to use the beloved Pytorch! <br>\nMy Previous tries were good, but they took **4 hours to run**\nNow after I made changes as the great francescopochetti, my excecution time has **come down to 10 minutes** (WOHOOOO!) <br>\nHave a look at the original work of Frances http://francescopochetti.com/pytorch-for-tabular-data-predicting-nyc-taxi-fares/\n","metadata":{}},{"cell_type":"markdown","source":"#### upvote if you find it useful. Sharing is the best way to learn!","metadata":{}},{"cell_type":"markdown","source":"Note: I will soon add all other references and will make the submission possible. <br>\n**Can someone please comment on why I hit on a local minima in the end**","metadata":{}},{"cell_type":"markdown","source":"Ideas to imporve:\n* Get a early stopping callback\n* Get learning rate scheduler\n* Make Network Deeper","metadata":{}},{"cell_type":"markdown","source":"## Simple imports","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\nimport pathlib\nimport matplotlib.pyplot as plt\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\npd.set_option('display.max_columns', 500)\nfrom collections import defaultdict\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.metrics import mean_squared_error\n","metadata":{"papermill":{"duration":7.781864,"end_time":"2022-01-25T15:39:09.265726","exception":false,"start_time":"2022-01-25T15:39:01.483862","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pytroch imports","metadata":{}},{"cell_type":"code","source":"from torch.nn import init\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nfrom torch.utils import data\nfrom torch.optim import lr_scheduler\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nfrom tqdm import tqdm # , # tqdm_notebook, # tnrange\nfrom tqdm.notebook import trange as tnrange # will change this to trange later \nfrom tqdm.notebook import tqdm as tqdm_notebook # will change this to tqdm later\ntqdm.pandas(desc='Progress')","metadata":{"papermill":{"duration":7.781864,"end_time":"2022-01-25T15:39:09.265726","exception":false,"start_time":"2022-01-25T15:39:01.483862","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import dataset","metadata":{"papermill":{"duration":0.015291,"end_time":"2022-01-25T15:39:09.296817","exception":false,"start_time":"2022-01-25T15:39:09.281526","status":"completed"},"tags":[]}},{"cell_type":"code","source":"df = pd.read_pickle('../input/ubiquant-market-prediction-half-precision-pickle/train.pkl')\ndf.head(2)","metadata":{"papermill":{"duration":16.88418,"end_time":"2022-01-25T15:39:26.19638","exception":false,"start_time":"2022-01-25T15:39:09.3122","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Setting investment id as categorical feature (just trying out!)","metadata":{}},{"cell_type":"code","source":"n_features = 300\nfeatures = [f'f_{i}' for i in range(n_features)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# setting as category feature\ndf['investment_id'] = df['investment_id'].astype('category')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Defining some helper functions to make life easy later","metadata":{}},{"cell_type":"code","source":"def split_features(df):\n    catf = ['investment_id']\n    numf = [col for col in df.columns if col not in catf]\n    \n    for c in catf: \n        df[c] = df[c].astype('category').cat.as_ordered()\n        df[c] = df[c].cat.codes + 1\n    \n    return catf, numf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def emb_init(x):\n    x = x.weight.data\n    sc = 2/(x.size(1)+1)\n    x.uniform_(-sc,sc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.loc[df['time_id']>400] # filter out old data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## make use of helper functions!","metadata":{}},{"cell_type":"code","source":"y = df['target']\ndf = df.drop(columns = ['target'], axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"catf, numf = split_features(df)\n\nprint(len(catf))\nprint(catf)\n\nprint(len(numf))\n# numf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.20, random_state=1)\nprint(X_train.shape, X_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_range = (y_train.min()*1.2, y_train.max()*1.2)\nprint(y_range)\n\ncat_sz = [(c, df[c].max()+1) for c in catf]\nprint(cat_sz)\n\nemb_szs = [(c, min(50, (c+1)//2)) for _,c in cat_sz]\nprint(emb_szs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define the Dataset by rewriting the data.Dataset module","metadata":{}},{"cell_type":"code","source":"class RegressionColumnarDataset(data.Dataset):\n    def __init__(self, df, cats, y):\n        self.dfcats = df[cats]\n        self.dfconts = df.drop(cats, axis=1)\n        \n        self.cats = np.stack([c.values for n, c in self.dfcats.items()], axis=1).astype(np.int64)\n        self.conts = np.stack([c.values for n, c in self.dfconts.items()], axis=1).astype(np.float32)\n        self.y = y.values.astype(np.float32)\n        \n    def __len__(self): return len(self.y)\n\n    def __getitem__(self, idx):\n        return [self.cats[idx], self.conts[idx], self.y[idx]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainds = RegressionColumnarDataset(X_train, catf, y_train)\nvalds = RegressionColumnarDataset(X_test, catf, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del X_train, X_test, y_train, y_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# params = {'batch_size': 1024,\n#           'shuffle': True,\n#           'num_workers': 2,\n#           'pin_memory': True}\n\ntraindl = data.DataLoader(trainds, batch_size = 1024, shuffle = False, num_workers = 2, pin_memory = True)\nvaldl = data.DataLoader(valds, batch_size = 2048, shuffle = True, num_workers = 2, pin_memory = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_cont = len(df.columns)-len(catf)\nn_cont","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del df,trainds, valds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training!","metadata":{}},{"cell_type":"markdown","source":"## The Neural Network!","metadata":{}},{"cell_type":"markdown","source":"This may look complex (it does to me!),  but actually this is quite simple. Have a nice read and check it out. <br>\nThe model mainly uses Embedding layers for the categorical variable (investmentid) and simple dense layers otherwise","metadata":{}},{"cell_type":"code","source":"class MixedInputModel(nn.Module):\n    def __init__(self, emb_szs, n_cont, emb_drop, out_sz, szs, drops, y_range, use_bn=True):\n        super().__init__()\n        \n        for i,(c,s) in enumerate(emb_szs): \n            assert c > 1, f\"cardinality must be >=2, got emb_szs[{i}]: ({c},{s})\"\n        \n        self.embs = nn.ModuleList([nn.Embedding(c, s) for c,s in emb_szs])\n        \n        for emb in self.embs: emb_init(emb)\n        n_emb = sum(e.embedding_dim for e in self.embs)\n        self.n_emb, self.n_cont = n_emb, n_cont\n        \n        # embeddings are done, now concatatenate \n        szs = [n_emb + n_cont] + szs\n        self.lins = nn.ModuleList([nn.Linear(szs[i], szs[i+1]) for i in range(len(szs)-1)])\n        self.bns = nn.ModuleList([nn.BatchNorm1d(sz) for sz in szs[1:]])\n        \n        # simple lines to make sure the weights are initialised in a kaiming distribution\n        for o in self.lins: nn.init.kaiming_normal_(o.weight.data)\n            \n        self.outp = nn.Linear(szs[-1], out_sz) # define output layer\n        nn.init.kaiming_normal_(self.outp.weight.data)\n\n        # define dropout layers\n        self.emb_drop = nn.Dropout(emb_drop)\n        self.drops = nn.ModuleList([nn.Dropout(drop) for drop in drops])\n        \n        # define batch normalisation layers\n        self.bn = nn.BatchNorm1d(n_cont)\n        self.use_bn, self.y_range = use_bn, y_range\n\n    def forward(self, x_cat, x_cont):\n        if self.n_emb != 0:\n            x = [e(x_cat[:,i]) for i,e in enumerate(self.embs)]\n            x = torch.cat(x, 1)\n            x = self.emb_drop(x)\n            \n        if self.n_cont != 0:\n            x2 = self.bn(x_cont)\n            x = torch.cat([x, x2], 1) if self.n_emb != 0 else x2\n            \n        for l,d,b in zip(self.lins, self.drops, self.bns):\n            x = F.silu(l(x))\n            if self.use_bn: x = b(x)\n            x = d(x)\n        x = self.outp(x)\n        \n#         if self.y_range:\n#             x = torch.sigmoid(x) # compresses values between 0 and 1\n#             x = x - torch.tensor(0.5, device = device)\n#             x = x * 2\n#             x = x*(self.y_range[1] - self.y_range[0])\n#             x = x+self.y_range[0]\n            \n        return x.squeeze()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m = MixedInputModel(emb_szs=emb_szs, \n                    n_cont=n_cont, \n                    emb_drop=0.04, \n                    out_sz=1, \n                    szs=[1000,500,250], \n                    drops=[0.001,0.01,0.01], \n                    y_range=y_range).to(device)\n\nopt = optim.Adam(m.parameters(), 5e-2)\nnum_epochs = 10\n\nlr = defaultdict(list)\ntloss = defaultdict(list)\nvloss = defaultdict(list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### check if the model looks good","metadata":{}},{"cell_type":"code","source":"m","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### overfit on one batch","metadata":{}},{"cell_type":"code","source":"device","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for cat, cont, y in traindl:\n#     print(cat, cont, y)\n    cat.cuda()\n    cont.to('cuda')\n    y.to(device)\n    print(cat.device, cont.device, y.device)\n    \n    break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat.device","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# compile the neural net\nnetwork = MixedInputModel(emb_szs=emb_szs, \n                    n_cont=n_cont, \n                    emb_drop=0.04, \n                    out_sz=1, \n                    szs=[1000,500,250], \n                    drops=[0.001,0.01,0.01], \n                    y_range=y_range)\n\noptimizer = optim.Adam(network.parameters(), lr=1e-1)\n\ntotal_loss = []\n\nfor i in range(100):\n    \n    # loss\n    loss = F.mse_loss(network(cat, cont), y)\n    total_loss.append(loss)\n    if (i%10 == 0):\n        print(\"Step\", i,\" loss:\", loss.item())\n\n    optimizer.zero_grad()\n    # backprop\n    loss.backward()  # update gradients\n    optimizer.step() # update weights using gradients to minimize loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(total_loss)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"1+1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fitting loop","metadata":{}},{"cell_type":"code","source":"def fit(model, train_dl, val_dl, loss_fn, opt, epochs = 3):\n    num_batch = len(train_dl)\n    for epoch in tnrange(epochs):   \n        \n        model.train()\n        y_true_train = list()\n        y_pred_train = list()\n        total_loss_train = 0          \n        \n        t = tqdm_notebook(iter(train_dl), leave=False, total=num_batch)\n        \n        for cat, cont, y in t:\n            cat = cat.cuda()\n            cont = cont.cuda()\n            y = y.cuda()\n            \n            t.set_description(f'Epoch {epoch}')\n            \n            opt.zero_grad()\n            pred = model(cat, cont)\n            loss = loss_fn(pred, y)\n            loss.backward()\n            lr[epoch].append(opt.param_groups[0]['lr'])\n            tloss[epoch].append(loss.item())\n            \n            opt.step()\n            \n            \n            t.set_postfix(loss=loss.item())\n            \n            y_true_train += list(y.cpu().data.numpy())\n            y_pred_train += list(pred.cpu().data.numpy())\n            total_loss_train += loss.item()\n            \n        train_acc = rmse(y_true_train, y_pred_train)\n        train_loss = total_loss_train/len(train_dl)\n        \n        if val_dl:\n            model.eval()\n            y_true_val = list()\n            y_pred_val = list()\n            total_loss_val = 0\n            for cat, cont, y in tqdm_notebook(val_dl, leave=False):\n                cat = cat.cuda()\n                cont = cont.cuda()\n                y = y.cuda()\n                pred = model(cat, cont)\n                loss = loss_fn(pred, y)\n                \n                y_true_val += list(y.cpu().data.numpy())\n                y_pred_val += list(pred.cpu().data.numpy())\n                total_loss_val += loss.item()\n                vloss[epoch].append(loss.item())\n                \n            valacc = rmse(y_true_val, y_pred_val)\n            valloss = total_loss_val/len(valdl)\n            print(f'Epoch {epoch}: train_loss: {train_loss:.4f} train_rmse: {train_acc:.4f} | val_loss: {valloss:.4f} val_rmse: {valacc:.4f}')\n        else:\n            print(f'Epoch {epoch}: train_loss: {train_loss:.4f} train_rmse: {train_acc:.4f}')\n    \n    return lr, tloss, vloss","metadata":{"execution":{"iopub.status.busy":"2022-02-21T04:42:39.660831Z","iopub.execute_input":"2022-02-21T04:42:39.661093Z","iopub.status.idle":"2022-02-21T04:42:39.67644Z","shell.execute_reply.started":"2022-02-21T04:42:39.661059Z","shell.execute_reply":"2022-02-21T04:42:39.675697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loop through training","metadata":{}},{"cell_type":"code","source":"lr, tloss, vloss = fit(model=m, train_dl=traindl, val_dl=valdl, loss_fn=F.mse_loss, opt=opt, epochs=num_epochs)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T04:43:16.198776Z","iopub.execute_input":"2022-02-21T04:43:16.199291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plot the results","metadata":{}},{"cell_type":"code","source":"t = [np.mean(tloss[el]) for el in tloss]\nv = [np.mean(vloss[el]) for el in vloss]\np = pd.DataFrame({'Train Loss': t, 'Validation Loss': v, 'Epochs': range(num_epochs)})\n\n_ = p.plot(x='Epochs', y=['Train Loss', 'Validation Loss'], \n           title='Train and Validation Loss over Epochs')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Can someone please comment why the loss has hit a minima?","metadata":{}},{"cell_type":"markdown","source":"# Upvote if useful!","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}